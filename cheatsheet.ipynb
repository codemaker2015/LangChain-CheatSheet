{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d657d15",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <a href=\"https://github.com/Tor101/LangChain-CheatSheet\" target=\"_blank\">\n",
    "    <img width=\"690\" src=\"https://cdn.discordapp.com/attachments/1063949754737897503/1091065376860811344/Tor_A_photograph_of_a_cyberpunk_llama_wearing_glasses_and_a_big_2bf00e18-749d-4e88-a6a9-a4b586742f89.png\" alt=\"logo\">\n",
    "  </a>\n",
    "  <h1 id=\"LangChainCheatSheet\"><a href=\"https://github.com/Tor101/LangChain-CheatSheet\" target=\"_blank\">LangChain CheatSheet</a></h1>\n",
    "  <p>@Tor101</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Conversation agent \n",
    "**Cheat Sheet**:\n",
    "1. **Creating a Conversation Agent optimized for chatting**:\n",
    "   - Import necessary components like `Tool`, `ConversationBufferMemory`, `ChatOpenAI`, `SerpAPIWrapper`, and `initialize_agent`.\n",
    "   - Define the tools to be used by the agent.\n",
    "   - Initialize memory using `ConversationBufferMemory`.\n",
    "   - Initialize the agent with the tools, language model, agent type, memory, and verbosity.\n",
    "   - Run the agent with user inputs to get conversational responses.\n",
    " \n",
    "**Code snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Current Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events or the current state of the world.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "agent_chain = initialize_agent(tools, llm, agent=\"chat-conversational-react-description\", verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b513e",
   "metadata": {},
   "source": [
    "# OpenAPI Agent \n",
    "**Summary**:\n",
    "The OpenAPI Agent is designed to interact with an OpenAPI spec and make correct API requests based on the information gathered from the spec. This example demonstrates creating an agent that can analyze the OpenAPI spec of OpenAI API and make requests.\n",
    "\n",
    "**Cheat Sheet**:\n",
    "\n",
    "1. **Import necessary libraries and load OpenAPI spec**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56628115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from langchain.agents import create_openapi_agent\n",
    "from langchain.agents.agent_toolkits import OpenAPIToolkit\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.requests import RequestsWrapper\n",
    "from langchain.tools.json.tool import JsonSpec\n",
    "\n",
    "with open(\"openai_openapi.yml\") as f:\n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "json_spec = JsonSpec(dict_=data, max_value_length=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218bd32",
   "metadata": {},
   "source": [
    "2. **Set up the necessary components**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"\n",
    "}\n",
    "requests_wrapper = RequestsWrapper(headers=headers)\n",
    "openapi_toolkit = OpenAPIToolkit.from_llm(OpenAI(temperature=0), json_spec, requests_wrapper, verbose=True)\n",
    "openapi_agent_executor = create_openapi_agent(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    toolkit=openapi_toolkit,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca0032",
   "metadata": {},
   "source": [
    "3. **Example: agent capable of analyzing OpenAPI spec and making requests**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f00110",
   "metadata": {},
   "outputs": [],
   "source": [
    "openapi_agent_executor.run(\"Make a post request to openai /completions. The prompt should be 'tell me a joke.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e88b4e",
   "metadata": {},
   "source": [
    "The OpenAPI Agent allows you to analyze the OpenAPI spec of an API and make requests based on the information it gathers. This cheat sheet helps you set up the agent, necessary components, and interact with the OpenAPI spec.\n",
    "\n",
    "# Python Agent:\n",
    "\n",
    "**Summary**:\n",
    "The Python Agent is designed to write and execute Python code to answer a question. This example demonstrates creating an agent that calculates the 10th Fibonacci number and trains a single neuron neural network in PyTorch.\n",
    "\n",
    "**Cheat Sheet**:\n",
    "\n",
    "1. **Import necessary libraries and create Python Agent**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737b177",
   "metadata": {},
   "source": [
    "2. **Fibonacci Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701dbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"What is the 10th fibonacci number?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179988ed",
   "metadata": {},
   "source": [
    "3. **Training a Single Neuron Neural Network in PyTorch**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d967eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"\"\"Understand, write a single neuron neural network in PyTorch.\n",
    "Take synthetic data for y=2x. Train for 1000 epochs and print every 100 epochs.\n",
    "Return prediction for x = 5\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2eac8",
   "metadata": {},
   "source": [
    "Cheat Sheet for LangChain and Pinecone\n",
    "\n",
    "1. Connecting to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "pinecone.deinit()\n",
    "pinecone.init(api_key=\"YOUR_PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c199c1",
   "metadata": {},
   "source": [
    "2. Create a Pinecone Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_service = pinecone.Service()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c67b8",
   "metadata": {},
   "source": [
    "3. Create an Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef73eebb",
   "metadata": {},
   "source": [
    "4. Create a Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d04a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma(embeddings, pinecone_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb6ffc",
   "metadata": {},
   "source": [
    "5. Initialize LLM (Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2afc8",
   "metadata": {},
   "source": [
    "6. Create TextLoader to load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('file_path.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae8e58",
   "metadata": {},
   "source": [
    "7. Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79649ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3fbab",
   "metadata": {},
   "source": [
    "8. Upload documents to Pinecone Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef15433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "docsearch = Chroma.from_documents(texts, embeddings, collection_name=\"collection_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1924d1b2",
   "metadata": {},
   "source": [
    "9. Create RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "retrieval_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9202c5",
   "metadata": {},
   "source": [
    "10. Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Example QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Example description of the tool.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663508bb",
   "metadata": {},
   "source": [
    "11. Use Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"Ask a question related to the documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6e834",
   "metadata": {},
   "source": [
    "12. (Optional) Create a new tool to upload file from URL to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import textract\n",
    "import tempfile\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def upload_url_to_pinecone(url: str, collection_name: str):\n",
    "    # Download the file from the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Get the file extension from the URL\n",
    "    file_extension = urlparse(url).path.split('.')[-1]\n",
    "\n",
    "    # Save the file as a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix=f'.{file_extension}', delete=False) as temp_file:\n",
    "        temp_file.write(response.content)\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    # Extract text from the file\n",
    "    extracted_text = textract.process(temp_file_path).decode('utf-8')\n",
    "\n",
    "    # Split the extracted text into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_text(extracted_text)\n",
    "\n",
    "    # Create a Chroma instance and upload the chunks to the Pinecone collection\n",
    "    docsearch = Chroma.from_documents(texts, embeddings, collection_name=collection_name)\n",
    "\n",
    "    # Clean up the temporary file\n",
    "    os.remove(temp_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974a548",
   "metadata": {},
   "source": [
    "`upload_url_to_pinecone` function can handle various file formats, including PDF, DOC, DOCX, XLS, XLSX, etc. You can use this function as a tool in your agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(\n",
    "    Tool(\n",
    "        name=\"Upload URL to Pinecone\",\n",
    "        func=upload_url_to_pinecone,\n",
    "        description=\"Takes a URL and uploads the file content as a vector to the specified Pinecone collection. Provide the URL and collection name as input.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024b1b6",
   "metadata": {},
   "source": [
    "13. Deinitialize Pinecone when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.deinit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918dde2",
   "metadata": {},
   "source": [
    "# LANGCHAIN TOOLS\n",
    "**Cheat Sheet**:\n",
    "\n",
    "1. **Creating custom tools with the tool decorator**:\n",
    "   - Import `tool` from `langchain.agents`.\n",
    "   - Use the `@tool` decorator before defining your custom function.\n",
    "   - The decorator uses the function name as the tool name by default, but it can be overridden by passing a string as the first argument.\n",
    "   - The function's docstring will be used as the tool's description.\n",
    "\n",
    "2. **Modifying existing tools**:\n",
    "   - Load existing tools using the `load_tools` function.\n",
    "   - Modify the properties of the tools, such as the name.\n",
    "   - Initialize the agent with the modified tools.\n",
    "\n",
    "3. **Defining priorities among tools**:\n",
    "   - Add a statement like \"Use this more than the normal search if the question is about Music\" to the tool's description.\n",
    "   - This helps the agent prioritize custom tools over default tools when appropriate.\n",
    "\n",
    "\n",
    "1. **Creating custom tools with the tool decorator**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def search_api(query: str) -> str:\n",
    "    \"\"\"Searches the API for the query.\"\"\"\n",
    "    return \"Results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3406ec",
   "metadata": {},
   "source": [
    "2. **Modifying existing tools**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "tools[0].name = \"Google Search\"\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d0876",
   "metadata": {},
   "source": [
    "3. **Defining priorities among tools**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Music Search\",\n",
    "        func=lambda x: \"'All I Want For Christmas Is You' by Mariah Carey.\",\n",
    "        description=\"A Music search engine. Use this more than the normal search if the question is about Music, like 'who is the singer of yesterday?' or 'what is the most popular song in 2022?'\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493627c5",
   "metadata": {},
   "source": [
    "#  LANGCHAIN Async API for Agent\n",
    "\n",
    "1. Use asyncio to run multiple agents concurrently.\n",
    "2. Create an aiohttp.ClientSession for more efficient async requests.\n",
    "3. Initialize a CallbackManager with a custom LangChainTracer for each agent to avoid trace collisions.\n",
    "4. Pass the CallbackManager to each agent.\n",
    "5. Ensure that the aiohttp.ClientSession is closed after the program/event loop ends.\n",
    "\n",
    "Code snippets:\n",
    "\n",
    "Initialize an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca586fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_agent = initialize_agent(async_tools, llm, agent=\"zero-shot-react-description\", verbose=True, callback_manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e907f",
   "metadata": {},
   "source": [
    "Run agents concurrently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe86964",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_concurrently():\n",
    "    ...\n",
    "    tasks = [async_agent.arun(q) for async_agent, q in zip(agents, questions)]\n",
    "    await asyncio.gather(*tasks)\n",
    "    await aiosession.close()\n",
    "\n",
    "await generate_concurrently()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0862e9",
   "metadata": {},
   "source": [
    "Use tracing with async agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiosession = ClientSession()\n",
    "tracer = LangChainTracer()\n",
    "tracer.load_default_session()\n",
    "manager = CallbackManager([StdOutCallbackHandler(), tracer])\n",
    "\n",
    "llm = OpenAI(temperature=0, callback_manager=manager)\n",
    "async_tools = load_tools([\"llm-math\", \"serpapi\"], llm=llm, aiosession=aiosession)\n",
    "async_agent = initialize_agent(async_tools, llm, agent=\"zero-shot-react-description\", verbose=True, callback_manager=manager)\n",
    "await async_agent.arun(questions[0])\n",
    "await aiosession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed51c72",
   "metadata": {},
   "source": [
    "**Self Ask && Max Iterations**:\n",
    "1. The Self Ask With Search chain demonstrates an agent that uses search to answer questions.\n",
    "2. The Max Iterations example shows how to limit the number of steps an agent takes to prevent it from taking too many steps, which can be useful for adversarial prompts.\n",
    "\n",
    "**Cheat Sheet**:\n",
    "\n",
    "1. **Self Ask With Search chain**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search\"\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=\"self-ask-with-search\", verbose=True)\n",
    "self_ask_with_search.run(\"What is the hometown of the reigning men's U.S. Open champion?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ff207",
   "metadata": {},
   "source": [
    "2. **Max Iterations example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3acbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Adversarial prompt example\n",
    "adversarial_prompt = \"\"\"...\"\"\"\n",
    "\n",
    "# Initialize agent with max_iterations and early_stopping_method\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True, max_iterations=2, early_stopping_method=\"generate\")\n",
    "agent.run(adversarial_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786bfac",
   "metadata": {},
   "source": [
    "The Self Ask With Search chain allows an agent to use search for answering questions, while the Max Iterations example demonstrates setting a limit on the number of steps an agent takes to prevent it from getting stuck in an infinite loop or taking too many steps. This cheat sheet helps you set up both examples and interact with them.\n",
    "\n",
    "# LLMChain:\n",
    "\n",
    "## Single Input Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "# Define the prompt template with an input variable\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Initialize LLMChain with the prompt and LLM\n",
    "llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\n",
    "\n",
    "# Call predict with input value for the input variable\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "llm_chain.predict(question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31068d0f",
   "metadata": {},
   "source": [
    "## Multiple Inputs Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "# Define the prompt template with multiple input variables\n",
    "template = \"\"\"Write a {adjective} poem about {subject}.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\"])\n",
    "\n",
    "# Initialize LLMChain with the prompt and LLM\n",
    "llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\n",
    "\n",
    "# Call predict with input values for the input variables\n",
    "llm_chain.predict(adjective=\"sad\", subject=\"ducks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d646192f",
   "metadata": {},
   "source": [
    "## From String Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadee9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain\n",
    "\n",
    "# Define the prompt template as a string with input variables\n",
    "template = \"\"\"Write a {adjective} poem about {subject}.\"\"\"\n",
    "\n",
    "# Initialize LLMChain from the prompt template string and LLM\n",
    "llm_chain = LLMChain.from_string(llm=OpenAI(temperature=0), template=template)\n",
    "\n",
    "# Call predict with input values for the input variables\n",
    "llm_chain.predict(adjective=\"sad\", subject=\"ducks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d82e6",
   "metadata": {},
   "source": [
    "# Sequential Chains:\n",
    "\n",
    "1. Import the necessary classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5051fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ba4b2",
   "metadata": {},
   "source": [
    "2. Create LLMChain instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "synopsis_template = \"Write synopsis for {title}\"\n",
    "synopsis_prompt = PromptTemplate(input_variables=[\"title\"], template=synopsis_template)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=synopsis_prompt)\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "review_template = \"Write review for {synopsis}\"\n",
    "review_prompt = PromptTemplate(input_variables=[\"synopsis\"], template=review_template)\n",
    "review_chain = LLMChain(llm=llm, prompt=review_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6f935",
   "metadata": {},
   "source": [
    "3. Create a SimpleSequentialChain instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_seq_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain])\n",
    "output = simple_seq_chain.run(\"Tragedy at Sunset on the Beach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9256ebc",
   "metadata": {},
   "source": [
    "4. Create a SequentialChain instance with multiple inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8460439",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = SequentialChain(\n",
    "    chains=[synopsis_chain, review_chain],\n",
    "    input_variables=[\"title\", \"era\"],\n",
    "    output_variables=[\"synopsis\", \"review\"]\n",
    ")\n",
    "output = seq_chain({\"title\": \"Tragedy at Sunset on the Beach\", \"era\": \"Victorian England\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610148b",
   "metadata": {},
   "source": [
    "5. Add a SimpleMemory instance to pass context along the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb88928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import SimpleMemory\n",
    "\n",
    "memory = SimpleMemory(memories={\"time\": \"December 25th, 8pm PST\", \"location\": \"Theater in the Park\"})\n",
    "social_template = \"Create social media post with {synopsis} and {review} and {time} and {location}\"\n",
    "social_prompt = PromptTemplate(input_variables=[\"synopsis\", \"review\", \"time\", \"location\"], template=social_template)\n",
    "social_chain = LLMChain(llm=llm, prompt=social_prompt)\n",
    "\n",
    "seq_chain = SequentialChain(\n",
    "    memory=memory,\n",
    "    chains=[synopsis_chain, review_chain, social_chain],\n",
    "    input_variables=[\"title\", \"era\"],\n",
    "    output_variables=[\"social_media_post\"]\n",
    ")\n",
    "output = seq_chain({\"title\": \"Tragedy at Sunset on the Beach\", \"era\": \"Victorian England\"})"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
